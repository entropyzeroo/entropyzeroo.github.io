<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>泊松图像编辑（Possion Image Edit）</title>
      <link href="/posts/8ad73f1a.html"/>
      <url>/posts/8ad73f1a.html</url>
      
        <content type="html"><![CDATA[<p><img src="http://q70i1gfoc.bkt.clouddn.com/possion/example.png" alt="Example"></p><p>虽然是2003年的文章了，但是由于其扎实的数学基础和至今看来都极其出色的效果，对每一个图像处理领域的学习者仍然是一篇值得一读的好文章。</p><a id="more"></a><p><strong>本文首发于本站，转载请注明来源</strong></p><p>文章内容为参考论文原文与网络相关内容的个人理解，如有错误，请在评论区指出。</p><blockquote><p>Poisson Image Editing - 2003</p></blockquote><h2 id="泊松图像编辑"><a href="#泊松图像编辑" class="headerlink" title="泊松图像编辑"></a>泊松图像编辑</h2><p>有兴趣的朋友可以详细了解一下泊松方程的来历及其数学原理，在这里给几个可供参考的链接。</p><blockquote><p><a href="https://www.cnblogs.com/herenzhiming/articles/5284514.html" target="_blank" rel="noopener">泊松方程的理论推导</a></p><p><a href="https://zhuanlan.zhihu.com/p/68349210" target="_blank" rel="noopener">从泊松方程的解法，聊到泊松图像融合</a></p><p><a href="https://blog.csdn.net/hjimce/article/details/45716603" target="_blank" rel="noopener">图像融合(1)Seamless cloning</a></p></blockquote><p>我看网络上很多相关解读都是直接从泊松融合入手，其实这并不是泊松图像编辑的本质内容，只能算是一个非常惊艳的应用方法。泊松图像编辑的本质是修改图像的梯度，然后通过泊松方程解最优化问题，从新的梯度恢复出修改后的图像。其梯度的修改可以包括很多种：改变梯度来源（泊松融合）、对梯度频带截断（去纹理）、调整不同通道梯度比例（改变颜色）等等，文章将会对这些应用都有基础的介绍。</p><p>在这里，以论文提到的内容为准，对论文涉及的相关数学知识简单梳理一下。当然，对本部分实在没有兴趣的朋友可以跳过，只看后面也可以大致理解泊松编辑的原理和方法。</p><h3 id="基于泊松方程的内插方法"><a href="#基于泊松方程的内插方法" class="headerlink" title="基于泊松方程的内插方法"></a>基于泊松方程的内插方法</h3><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/1.png" alt="S"></p><p>从上面两个图开始说起，假设左边的$S$本来是一副好好的图片，那这时候我们在上面扣了一个洞，这个洞的样子大概就是右边这个图片，那这个$\Omega$呢就表示这个洞的区域，$\partial\Omega$则表示这个洞的边界。造成了这样的结果，是谁的错已经不重要了，总之我们现在想要把这个图像恢复要原来的样子，也就是想把这个洞填上。那现在没有任何可以参照的东西，剩下的只有带了洞的$S$，唯一能够直接拿来用的也只有这个洞的边界像素值$\partial\Omega$我们还是知道的，想要填补这个洞，我们希望填出来的内容满足下面两个条件：</p><ul><li>填补内容要尽可以平滑</li><li>填补内容的边界像素值和现有的$S$一致，即要无缝过度</li></ul><p>那要满足上述两个要求，就得到了以下的数学表达：</p><script type="math/tex; mode=display">\underset{f}{min}\iint_{\Omega}\left|\nabla f\right|^2\ \text{ with }\ f|_{\partial\Omega}=f^*|_{\partial\Omega} \tag1</script><p>其中$\nabla$是一阶微分，即梯度算子。那这个式子其实就是按照上面两个条件列出来的，意思就是在$\Omega$区域梯度尽可能小（平滑），满足边界上像素相等的约束条件（无缝过度）。而上述最优化问题的解满足欧拉-拉格朗日方程：</p><script type="math/tex; mode=display">\Delta f=0\ \text{ over }\  \Omega \ \text{ with }\  f|_{\partial\Omega}=f^*|_{\partial\Omega} \tag2</script><p>其中$\Delta$表示二阶微分（直角坐标系下的散度<strong>div</strong>），即拉普拉斯算子，也就是二阶梯度$\nabla ^2$，学过图像处理的朋友应该很熟悉。而这个时候的边界条件就称为<strong>狄利克雷边界(Dirichlet boundary)</strong>。众所周知，取得一阶微分极值时，二阶微分等于0。所以要在梯度取最小的时候，就要在使得散度为0，因此就有了上式。已知区域$\Omega$内的散度都是0，以及边界上已知的像素值，即可求出$\Omega$内部所有像素值，根据以上方程填充的图像将会是以下结果。</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/2.png" alt="lap"></p><p>上图从左到右分别为原图、填充区域、复原图。</p><p><del>看到这里想必有朋友已经怒拍键盘，看了半天你就给我糊一马赛克上去？</del></p><p>平心而论，有一说一，这个结果其实已经充分满足我们给出的条件了：1、填补内容尽可能平滑；2、颜色也在边界像素点的约束下与背景保持一致。</p><p>那之所以最后出现这样不堪入目的结果，是因为我们没有告诉它里面要填些啥东西，所以大家先坐下来继续看。这个时候我们就要稍微引导它一下，给它带入正规，这里就要用到<strong>引导向量场(guidance vector field)</strong>，$\mathbf{v}$。</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/3.png" alt="V"></p><p>好了，上图多了$\mathbf{v}$和$\mathbf{g}$ ，其中$\mathbf{v}$是引导向量场，也就是梯度场，可以理解为从$\mathbf{g}$中导出的。其实我们并不关心$\mathbf{g}$是什么，我们只关心$\mathbf{v}$是什么，它可以是从通过一张名为$\mathbf{g}$的图中导出的，也可以是对原图$S$中被扣掉的那部分里拿过来的，甚至可以是你随意生成的。不管它是怎么来的，总之现在我们又多了一个$\mathbf{v}$，所以我们对填补任务又多出了新的要求：</p><ul><li>填补内容的梯度要尽可以与$\mathbf{v}$接近</li><li>填补内容的边界像素值和现有的$S$一致，即要无缝过度</li></ul><p>与式(1)和式(2)相对应的，现在我们得出了新的最优化问题：</p><script type="math/tex; mode=display">\underset{f}{min}\iint_{\Omega}\left|\nabla f-\mathbf{v}\right|^2\ \text{ with } \ f|_{\partial\Omega}=f^*|_{\partial\Omega} \tag3</script><p>同样的，该问题在狄利克雷边界约束下的泊松方程为：</p><script type="math/tex; mode=display">\Delta f=\text{div}\mathbf{v}\ \text{ over }\  \Omega \ \text{ with } \ f|_{\partial\Omega}=f^*|_{\partial\Omega} \tag4</script><p>与之前相比，改变的仅仅是等式右边变成了由$\mathbf{v}$计算的散度值而已。至于你选择用什么$\mathbf{v}$，那就根据你的需求来了。在上述式子的基础上，同样的图像，再给出一个填充的实例。</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/4.png" alt="lap2"></p><p>上图从左到右分别为原图、填充区域、复原图。</p><blockquote><p>—-不是，整半天这和原图有什么区别？</p><p>—-唉，好像左右镜像了一下。</p><p>—-???</p><p>—-卧槽？你少拿原图来骗我</p></blockquote><p>不知道在看了结果以后，大家脑子里有没有这样的小剧场。在这里，我用的引导梯度$\mathbf{v}$就是框选区域五边形内的$\mathbf{v}$进行左右镜像后的$\mathbf{v}$。由于本来就是从原图中选择的内容，边缘梯度变化和颜色值本来就非常接近，因此在经过泊松方程重建后几乎可以做到天衣无缝的图像编辑效果。</p><p>以上就是泊松图像编辑的核心内容。因为本人的数学功底比较一般，对几个方程并没有更加深入的剖析，也没有进行相关数学理论的推导，只是结合自己的理解进行了简单的分析。总之，对泊松图像编辑的总结就是：</p><ul><li>本质：修改待插入区域的梯度</li><li>方法：解泊松方程</li></ul><h3 id="离散泊松方程解法"><a href="#离散泊松方程解法" class="headerlink" title="离散泊松方程解法"></a>离散泊松方程解法</h3><p>看了以上部分，应该多多少少对泊松图像编辑有了基本的认识，本部分将讲一讲离散泊松方程解法。</p><p>$\Delta f=\text{div}\mathbf{v} \text{ over }  \Omega  \text{ with }  f|_{\partial\Omega}=f^*|_{\partial\Omega}$</p><p>等式(4)就是泊松方程的核心了，在这里用一个简单的例子说明一下如何利用构建上述泊松方程。</p><p>以一个简单的$4\times4$大小的图像进行说明，假设$X=\begin{bmatrix}x_1 &amp; x_2 &amp; x_3 &amp; x_4 \\\ x_5 &amp; x_6 &amp; x_7 &amp; x_8 \\\ x_9 &amp; x_{10} &amp; x_{11} &amp; x_{12} \\\ x_{13} &amp; x_{14} &amp; x_{15} &amp; x_{16} \end{bmatrix}$.</p><p>其中$x_6,x_7,x_{10},x_{11}$四个点所构成的$2\times2$大小的矩阵为我们所要填充的部分，像素值未知，但是散度已知（修改后的梯度所导出的散度）。其余位置为填充边界，像素值已知。因此只需要对上述4个待求点列出泊松方程即可：</p><p>$\begin{cases}x_2+x_5+x_7+x_{10}-4x_6=\text{div}x_6 \\\ x_3+x_6+x_8+x_{11}-4x_7=\text{div}x_7 \\\ x_6+x_9+x_{11}+x_{14}-4x_{10}=\text{div}x_{10} \\\ x_7+x_{10}+x_{12}+x_{15}-4x_{11}=\text{div}x_{11}\end{cases}$</p><p>如上文所说，上述方程组中，右侧散度已知，左侧只有4个待求点函数值未知，4个方程刚好可以求解。</p><p>上述解法利用了待求区域周围一圈的像素值，即满足狄利克雷边界条件，值得一提的是，求解泊松方程还有一种约束条件，纽曼边界。</p><ul><li><p><strong>Neumann 边界</strong>，译为纽曼边界或黎曼边界，给出函数在边界处的二阶导数值</p></li><li><p><strong>Dirichlet 边界</strong>，狄利克雷边界，给出边界处函数在边界处的实际值</p></li></ul><p>不管怎么说，将上述求方程组的问题，用矩阵形式表示后即为$Ax=B$.</p><p>最后将求出来的$x$代回图像中对应的位置即可。</p><h3 id="Matlab实现"><a href="#Matlab实现" class="headerlink" title="Matlab实现"></a>Matlab实现</h3><p>如果你只想直接调用函数测试，可以在python/C++直接使用OpenCV提供的接口。</p><p>如果你想研究一下代码及具体实现方式，那可以继续看一下这部分的内容。</p><p>我自己在用Matlab实现算法的时候，遇到了很多问题，例如出现边界过渡不自然等问题。简单的搜了一下度娘，关于详细实现部分居然很少，或者是代码不忍直视，也没去试效果。后来又在Github上找，找到的代码也不尽人意，于是还是自己动手，丰衣足食。</p><blockquote><p><a href="https://ww2.mathworks.cn/matlabcentral/fileexchange/62287-poisson-image-editing" target="_blank" rel="noopener">Matlab官网别人上传的代码</a></p><p><a href="https://github.com/CaptainSharf/Poisson-Image-Editing" target="_blank" rel="noopener">Github别人的repo</a></p></blockquote><p>上面两个是我在编程过程中参考的代码，第一个代码在我个人实验的过程中发现其边界处理仍存在问题，第二个代码我借用了其UI界面部分的代码，用于选择融合区域并创建Mask。</p><p>我在尽可能保证代码能较好复现结果的前提下，增加了代码的可读性，并附上了比较详细的注释，有兴趣的朋友可以看看。代码包括接下来会讲到的各种应用的demo。</p><blockquote><p><a href="">泊松图像编辑Matlab实现</a></p></blockquote><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="Seamless-Cloning"><a href="#Seamless-Cloning" class="headerlink" title="Seamless Cloning"></a>Seamless Cloning</h3><p>这就是广为人知的泊松融合，有一副背景图像$S$和另一幅源图像$\mathbf{g}$，想要把$\mathbf{g}$或$\mathbf{g}$中的一部分插入到$S$中，只需要用$\mathbf{g}$的梯度作为引导梯度，解式(4)中的泊松方程即可。</p><p>此时的引导向量场$\mathbf{v}=\nabla\mathbf{g}$。融合结果如下图所示：</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/5.png" alt="ss1"></p><p>看一下效果，嗯，一切都在计划之中。</p><p>但是，这是因为融合图像的背景都是水，看起来差不多，所以融合的结果也比较好。如果仔细看的话，还是能看到不少融合区域周围有一定的融合痕迹。而当你要融合两种背景纹理特征差异比较大的物体时，你就会发现出现问题了。以本文的标题图片为例：</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/6.png" alt="ss2"></p><p>背景木板上其实是有很多细节的，而我们截图的文字背景是非常平滑的单一颜色，因此引导梯度场可以说是基本都为0，最后导致了融合后的文字背景也非常平滑。那这样的结果显然不是我们期望的，我们希望既能将文字添加上去，又能保留木板上的纹理细节。论文中当然也提到了这部分，就是混合梯度融合。</p><h3 id="Mixing-Gradients-Seamless-Cloning"><a href="#Mixing-Gradients-Seamless-Cloning" class="headerlink" title="Mixing Gradients Seamless Cloning"></a>Mixing Gradients Seamless Cloning</h3><p>那要怎么做呢？还记得之前说的，图像编辑的本质是修改梯度。我们现在的做法是从一张图片中提取梯度，然后完全覆盖到背景图上去，现在我们想要保留两者中各自的高频细节，那相应的引导向量场就用两者较大的部分。</p><p><img src="https://img-blog.csdnimg.cn/20200317183956761.png" alt="formula"></p><p>这一步需要先判断梯度的大小，然后取绝对值较大的梯度，然后根据新的梯度求取散度，再代入泊松方程求解，就可以得到以下结果。</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/7.png" alt="mss"></p><p>OK，大功告成，这样看起来就舒服多了。当然，实际使用的时候还是要根据需求来，并不一定说混合梯度的效果绝对要比单一梯度效果好。如果有需求，甚至可以设计自适应加权的梯度修改方法。</p><h3 id="Texture-flattening"><a href="#Texture-flattening" class="headerlink" title="Texture flattening"></a>Texture flattening</h3><p>到了现在，大家应该都明白如何操作梯度来得到自己想要的结果了。那去纹理的意思显然就是把图像中的一部分纹理移除，同样的，在梯度修改时，我们直接放弃一部分梯度，所以对待求区域的每一个点作如下操作。</p><p>$\text{for all}  \mathbf{x} \in \Omega,  \mathbf{v}(\mathbf{x})=M(\mathbf{x})\nabla f^*(\mathbf{x})$,</p><p>其中$M(\mathbf{x})$是一个二值模板，即0-1，指示保留哪些梯度，删除哪些梯度。最简单的二值模板就是直接给一个阈值，只保留大于该阈值的信息或者小于该阈值的信息，下面给一个简单的例子：</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/8.png" alt="Texture flattening 1"></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grad(<span class="built_in">abs</span>(grad)&lt;<span class="number">0.08</span>)=<span class="number">0</span>;</span><br></pre></td></tr></table></figure><blockquote><p>emmmmm、抱歉、我的、打扰了、没控制好力度、重来、</p></blockquote><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/9.png" alt="Texture flattening 2"></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grad(<span class="built_in">abs</span>(grad)&lt;<span class="number">0.02</span>)=<span class="number">0</span>;</span><br></pre></td></tr></table></figure><p>嗯，大致效果就是这样，根据你截断的力度，会得到不同的编辑效果。直接用常数作阈值肯定是不合适的，更进一步也完全可以用自适应的方法选择保留哪部分的内容。</p><p>有点类似于边缘保持滤波器的效果，控制好力度或许能用来美颜这样？</p><h3 id="Local-illumination-changes"><a href="#Local-illumination-changes" class="headerlink" title="Local illumination changes"></a>Local illumination changes</h3><p>可用于图像局部对比度提高或降低，类似于HDR的应用。</p><p>这里是对梯度域作了一个非线性变换，引用了以下这个论文，我也没仔细看，有兴趣的朋友可以自己去研究一下。<a href="https://www.researchgate.net/profile/Dani_Lischinski/publication/2530899_Gradient_Domain_High_Dynamic_Range_Compression/links/00463524c3f5cd2e34000000/Gradient-Domain-High-Dynamic-Range-Compression.pdf" target="_blank" rel="noopener">Gradient Domain High Dynamic Range Compression</a></p><p>总之大致效果是这样的，还是很不错的：</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/10.png" alt="Local illumination changes"></p><h3 id="Local-color-changes"><a href="#Local-color-changes" class="headerlink" title="Local color changes"></a>Local color changes</h3><p>用于改变图像局部颜色值，好处在于对改变目标不用框选很精确的区域，然后结果也比较自然。</p><p>方法很简单，就是对RGB三个通道的梯度值进行一定的增强和衰减，或者是替换等等各种操作，你喜欢怎么来就怎么来。直接给图：</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/11.png" alt="Local color changes"></p><p>左边原图</p><p>中间是论文给出的示例，梯度变化为：R*1.5,G/2,B/2</p><p>右边是我测试的结果，梯度变化为：R用B替换</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">效果<span class="number">1</span>：</span><br><span class="line">grad(:,:,<span class="number">1</span>)=grad(:,:,<span class="number">1</span>)*<span class="number">1.5</span>;</span><br><span class="line">grad(:,:,<span class="number">2</span>)=grad(:,:,<span class="number">2</span>)/<span class="number">2</span>;</span><br><span class="line">grad(:,:,<span class="number">3</span>)=grad(:,:,<span class="number">3</span>)/<span class="number">2</span>;</span><br><span class="line">效果<span class="number">2</span>：</span><br><span class="line">grad(:,:,<span class="number">1</span>)=grad(:,:,<span class="number">3</span>);</span><br><span class="line">grad(:,:,<span class="number">2</span>)=grad(:,:,<span class="number">2</span>);</span><br><span class="line">grad(:,:,<span class="number">3</span>)=grad(:,:,<span class="number">3</span>);</span><br></pre></td></tr></table></figure><blockquote><p>显然是右边的颜色比较好看，能否苟同？</p></blockquote><h3 id="Seamless-tiling"><a href="#Seamless-tiling" class="headerlink" title="Seamless tiling"></a>Seamless tiling</h3><p>论文给出的最后一个应用，对于一个矩形类内容的图像，类似补丁的一块，可以产生无缝拼接的效果。操作方法也很简单，修改图像的边界像素，就是第一行/列，最后一行/列。如：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">row_1 = <span class="number">0.5</span>*(row_1+row_end);</span><br><span class="line">row_end = row_1;</span><br><span class="line">col_1 = <span class="number">0.5</span>*(col_1+col_end);</span><br><span class="line">col_end = col_1;</span><br></pre></td></tr></table></figure><p>即上下边界和左右边界设为原图像的边界和的一半。</p><p>然后引导向量场仍然使用原图像的梯度场。这样得到的结果就可以用来无缝拼接，如下图所示：</p><p><img src="http://q70i1gfoc.bkt.clouddn.com/PossionImageEdit/12.png" alt="Seamless tiling"></p><p>以上分别为直接拼接和泊松拼接的效果，看起来还是不错的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不提了，都在上面了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p>Poisson Image Editing - 2003</p><p>网络相关内容已在文中给出链接</p><p>示例图片均来源于论文，木板背景来源百度图片，侵删</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PossionEdit </tag>
            
            <tag> 泊松融合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RGB-NIR图像融合</title>
      <link href="/posts/4286dff3.html"/>
      <url>/posts/4286dff3.html</url>
      
        <content type="html"><![CDATA[<p>整理RGB-NIR图像融合相关内容</p><a id="more"></a><h2 id="融合数据集"><a href="#融合数据集" class="headerlink" title="融合数据集"></a>融合数据集</h2><p><a href="http://matthewalunbrown.com/nirscene/nirscene.html" target="_blank" rel="noopener">RGB-NIR Scene Dataset</a></p><h2 id="NIR相关介绍"><a href="#NIR相关介绍" class="headerlink" title="NIR相关介绍"></a>NIR相关介绍</h2><h3 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h3><p>相关研究做的比较多的作者。</p><blockquote><p><em>C Fredembach</em></p></blockquote><h3 id="NIR特性及获取方法等"><a href="#NIR特性及获取方法等" class="headerlink" title="NIR特性及获取方法等"></a>NIR特性及获取方法等</h3><h4 id="1、Colouring-the-Near-Infrared-2008-Paper"><a href="#1、Colouring-the-Near-Infrared-2008-Paper" class="headerlink" title="1、Colouring the Near-Infrared - 2008 -Paper"></a>1、Colouring the Near-Infrared - 2008 -<a href="https://infoscience.epfl.ch/record/129419/files/IR_colour.pdf" target="_blank" rel="noopener">Paper</a></h4><h4 id="2、Material-based-object-segmentation-using-near-infrared-information-2010-Paper"><a href="#2、Material-based-object-segmentation-using-near-infrared-information-2010-Paper" class="headerlink" title="2、Material-based object segmentation using near-infrared information - 2010 -Paper"></a>2、Material-based object segmentation using near-infrared information - 2010 -<a href="https://infoscience.epfl.ch/record/153245/files/Neda_Salamati_CIC18.pdf" target="_blank" rel="noopener">Paper</a></h4><h4 id="3、Designing-color-filter-arrays-for-the-joint-capture-of-visible-and-near-infrared-images-2009-Paper"><a href="#3、Designing-color-filter-arrays-for-the-joint-capture-of-visible-and-near-infrared-images-2009-Paper" class="headerlink" title="3、Designing color filter arrays for the joint capture of visible and near-infrared images - 2009 -Paper"></a>3、Designing color filter arrays for the joint capture of visible and near-infrared images - 2009 -<a href="http://ivrlwww.epfl.ch/alumni/fredemba/papers/LFVS_ICIP09.pdf" target="_blank" rel="noopener">Paper</a></h4><h4 id="4、Correlation-based-joint-acquisition-and-demosaicing-of-visible-and-near-infrared-images-2011-Paper"><a href="#4、Correlation-based-joint-acquisition-and-demosaicing-of-visible-and-near-infrared-images-2011-Paper" class="headerlink" title="4、Correlation-based joint acquisition and demosaicing of visible and near-infrared images - 2011 -Paper"></a>4、Correlation-based joint acquisition and demosaicing of visible and near-infrared images - 2011 -<a href="https://infoscience.epfl.ch/record/168897/files/SadeghipoorICIP11.pdf" target="_blank" rel="noopener">Paper</a></h4><h2 id="融合算法"><a href="#融合算法" class="headerlink" title="融合算法"></a>融合算法</h2><h3 id="1、Adaptive-near-infrared-and-visible-fusion-for-fast-image-enhancement-2019"><a href="#1、Adaptive-near-infrared-and-visible-fusion-for-fast-image-enhancement-2019" class="headerlink" title="1、Adaptive near-infrared and visible fusion for fast image enhancement - 2019"></a>1、Adaptive near-infrared and visible fusion for fast image enhancement - 2019</h3><p><img src="http://q70i1gfoc.bkt.clouddn.com/nirfusion/awad.png" alt="awad"></p><p>RGB图像转Ycbcr空间，拿出Y层（亮度层）与NIR图像融合。用高通滤波器提取Y和NIR的高频成分，即细节和纹理。用局部对比度估计公式估计Y和NIR的局部对比度，作为细节融合的权重。融合的细节与Y层低频成分相加，最后再还原到RGB图像。</p><h3 id="2、Enhancing-Photographs-with-Near-Infrared-Images-2008-CVPR"><a href="#2、Enhancing-Photographs-with-Near-Infrared-Images-2008-CVPR" class="headerlink" title="2、Enhancing Photographs with Near Infrared Images - 2008 CVPR"></a>2、Enhancing Photographs with Near Infrared Images - 2008 CVPR</h3><p><img src="http://q70i1gfoc.bkt.clouddn.com/nirfusion/cvpr2008.png" alt="cvpr2008"></p><p>RGB空间转HSV空间，对V层与NIR图像融合。利用Haar小波变换将图像分解到不同频率子带下，分别融合低频成分和高频成分。在低频利用梯度引导做直方图匹配，在高频上对两个图像的子带alpha加权，最后反变换重建融合图像。本文还提到了如何搭建相机环境以获取NIR图像。</p><h3 id="3、RGB–NIR-Image-Enhancement-by-Fusing-Bilateral-and-Weighted-Least-Squares-Filters-2017"><a href="#3、RGB–NIR-Image-Enhancement-by-Fusing-Bilateral-and-Weighted-Least-Squares-Filters-2017" class="headerlink" title="3、RGB–NIR Image Enhancement by Fusing Bilateral and Weighted Least Squares Filters - 2017"></a>3、RGB–NIR Image Enhancement by Fusing Bilateral and Weighted Least Squares Filters - 2017</h3><p><img src="http://q70i1gfoc.bkt.clouddn.com/nirfusion/bfwls.png" alt="bfwls"></p><p>RGB空间转Ycbcr空间，对Y层与NIR图像融合。同时使用WLS(加权最小二次滤波器)与BF(双边滤波器)对图像滤波得到低频图像，利用原图与低频差分获取高频信息。计算用两种滤波器提取的NIR图像高频成分，替换Y图像中的高频成分，获得融合图像。</p><h3 id="4、Color-Image-Dehazing-Using-The-Near-infrared-2009"><a href="#4、Color-Image-Dehazing-Using-The-Near-infrared-2009" class="headerlink" title="4、Color Image Dehazing Using The Near-infrared - 2009"></a>4、Color Image Dehazing Using The Near-infrared - 2009</h3><p><img src="http://q70i1gfoc.bkt.clouddn.com/nirfusion/wlsmr.png" alt="wlsmr"></p><p>同样将从RGB图像中提取V作为亮度表示。利用WLS(最小二次滤波器)将图像分别到文章中提到的多分辨率表示(<em>multiresolution representation</em>)，这里的多分辨率指图像平滑尺度不同，但是应该没改变图像尺寸。然后同样获得低频成分(<em>approximation images</em>)与高频成分(<em>detail images</em>)，对高频成分按两者较大值融合，低频成分选择V的低频，放弃NIR图像的低频。然后从多分辨率表示重建回融合图像。</p><h3 id="5、Combining-visible-and-near-infrared-images-for-realistic-skin-smoothing-2009"><a href="#5、Combining-visible-and-near-infrared-images-for-realistic-skin-smoothing-2009" class="headerlink" title="5、Combining visible  and near-infrared images for realistic skin smoothing - 2009"></a>5、Combining visible  and near-infrared images for realistic skin smoothing - 2009</h3><p><img src="http://q70i1gfoc.bkt.clouddn.com/nirfusion/bf.png" alt="bf"></p><p>本文研究重点在于利用NIR图像对人脸去噪，额，也就是美颜。文章详细探讨了NIR波段对人皮肤的特征、缺陷、构造等不同区域的响应。融合方法依旧是将RGB图像转化到亮度表示Y，利用双边滤波器对图像滤波，得到平滑层，继而得到细节层。直接融合Y的平滑层和NIR图像的细节层就是最终结果，没有自适应的权重。</p><h3 id="6、Near-infrared-guided-color-image-dehazing-2013-ICIP"><a href="#6、Near-infrared-guided-color-image-dehazing-2013-ICIP" class="headerlink" title="6、Near-infrared guided color image dehazing - 2013 ICIP"></a>6、Near-infrared guided color image dehazing - 2013 ICIP</h3><h3 id="7、Near-infrared-fusion-via-color-regularization-for-haze-and-color-distortion-removals-2017"><a href="#7、Near-infrared-fusion-via-color-regularization-for-haze-and-color-distortion-removals-2017" class="headerlink" title="7、Near-infrared fusion via color regularization for haze and color distortion removals - 2017"></a>7、Near-infrared fusion via color regularization for haze and color distortion removals - 2017</h3><h3 id="8、"><a href="#8、" class="headerlink" title="8、"></a>8、</h3>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NIR </tag>
            
            <tag> 图像融合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>时间分割线</title>
      <link href="/posts/63838fc1.html"/>
      <url>/posts/63838fc1.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://i0.hdslb.com/bfs/article/4adb9255ada5b97061e610b682b8636764fe50ed.png" alt=""></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>白平衡算法之Gray Edge</title>
      <link href="/posts/df87526a.html"/>
      <url>/posts/df87526a.html</url>
      
        <content type="html"><![CDATA[<h3 id="Gray-Edge假设及其一般性计算框架"><a href="#Gray-Edge假设及其一般性计算框架" class="headerlink" title="Gray Edge假设及其一般性计算框架"></a>Gray Edge假设及其一般性计算框架</h3><p>&emsp;&emsp;有关MaxRGB，GrayWorld算法的回顾，可以参考<a href="/posts/26a617fd.html" title="白平衡算法">白平衡算法</a>。<a href="https://blog.csdn.net/weixin_43194305/article/details/101794682" target="_blank" rel="noopener">CSDN原始发布</a>。</p><h4 id="Gray-Edge假设"><a href="#Gray-Edge假设" class="headerlink" title="Gray Edge假设"></a>Gray Edge假设</h4><p>&emsp;&emsp;MaxRGB，GrayWorld等算法都是基于原始图像的特征提出的。之后，Weijer等人通过观察对立颜色空间的图像颜色导数分布，发现图像的颜色导数在对立颜色空间呈一定规则。这里说到的对立颜色空间是根据颜色对抗学说建立的一个颜色空间，对立颜色空间的获得是从RGB空间做一个变换得到，具体有关对立颜色空间和颜色对抗学说的内容可移步<a href="https://en.wikipedia.org/wiki/Opponent_process" target="_blank" rel="noopener"><strong>维基百科</strong></a>。</p><p>&emsp;&emsp;根据图像颜色在这个空间上的一些特性，Weijer提出了一种新的Gray Edge假说：场景中所有物理表面的平均反射的差分是无色差的。同时，为了兼容几大基本算法，闵可夫斯基范式也被引入，得到了一个更为通用的颜色恒常性计算的算法框架，其最终的数学形式为：</p><script type="math/tex; mode=display">(\int |\frac{\partial^nf^\sigma(X)}{\partial X^n}|^p dX)^{1/p}=ke^{n,p,\sigma}\tag1</script><p>其中$f^n=f\otimes G^\sigma$，表示图像$f$与高斯滤波器$G^\sigma$的卷积；$\partial^n/\partial X^n$表示$n$阶导数过程。选择不同的$n,p,\sigma$参数，分别包含了几大基础算法，如下表所示。</p><center><b>Gray Edge框架下的颜色恒常性算法</b></center><div class="table-container"><table><thead><tr><th style="text-align:left">算法</th><th style="text-align:left">参数</th><th style="text-align:left">公式</th></tr></thead><tbody><tr><td style="text-align:left">Gray World</td><td style="text-align:left">$e^{0,1,0}$</td><td style="text-align:left">$(\int f(X)dX)=ke$</td></tr><tr><td style="text-align:left">MaxRGB</td><td style="text-align:left">$e^{0,\infty,0}$</td><td style="text-align:left">$(\int \</td><td>f(X)\</td><td>^\infty dX)^\frac{1}{\infty}=ke$</td></tr><tr><td style="text-align:left">Shades of Gray</td><td style="text-align:left">$e^{0,p,0}$</td><td style="text-align:left">$(\int \</td><td>f(X)\</td><td>^p dX)^\frac{1}{p}=ke$</td></tr><tr><td style="text-align:left">Gauss Gray World</td><td style="text-align:left">$e^{0,p,\sigma}$</td><td style="text-align:left">$(\int \</td><td>f^\sigma(X)\</td><td>^pdX)^\frac{1}{p}=ke$</td></tr><tr><td style="text-align:left">1st Order Gray Edge</td><td style="text-align:left">$e^{1,p,\sigma}$</td><td style="text-align:left">$(\int \</td><td>f^\sigma_X(X)\</td><td>^pdX)^\frac{1}{p}=ke$</td></tr><tr><td style="text-align:left">max Edge</td><td style="text-align:left">$e^{1,\infty,\sigma}$</td><td style="text-align:left">$(\int \</td><td>f^\sigma_X(X)\</td><td>^\infty dX)^\frac{1}{\infty}=ke$</td></tr><tr><td style="text-align:left">2nd Order Gray Edge</td><td style="text-align:left">$e^{2,p,\sigma}$</td><td style="text-align:left">$(\int\</td><td>f^\sigma_{XX}(X)\</td><td>^pdX)^\frac{1}{p}=ke$</td></tr></tbody></table></div><p>&emsp;&emsp;当然，即使Gray World假设来源于对立颜色空间，但是所有处理依旧是在RGB颜色空间进行的。Gray Edge的重要意义在于把原来在0阶图像上进行的颜色恒常性计算推广到了高阶上。</p><h4 id="Matlab代码"><a href="#Matlab代码" class="headerlink" title="Matlab代码"></a>Matlab代码</h4><p>以下是自己写的一个简易版的代码，固定了只求1阶导数。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">out</span> = <span class="title">GrayEdge</span><span class="params">(im,p,sigma)</span></span></span><br><span class="line"><span class="comment">% Image should be normalized to 0-1</span></span><br><span class="line"><span class="comment">% n    图像阶数         固定1,这里图像导数的来源有待考证</span></span><br><span class="line"><span class="comment">% p    Minkowski范数    默认6</span></span><br><span class="line"><span class="comment">% alpha 高斯滤波尺度    默认2</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line">out = im;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ~exist(<span class="string">'p'</span>,<span class="string">'var'</span>)</span><br><span class="line">    p=<span class="number">6</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">if</span> ~exist(<span class="string">'alpha'</span>,<span class="string">'var'</span>)</span><br><span class="line">    sigma=<span class="number">2</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">k = fspecial(<span class="string">'gaussian'</span>,<span class="built_in">floor</span>(sigma*<span class="number">3</span>+<span class="number">0.5</span>),sigma);<span class="comment">%创建高斯模板</span></span><br><span class="line">im_G = imfilter(im,k,<span class="string">'replicate'</span>);<span class="comment">%高斯滤波</span></span><br><span class="line">im_edge = gradient(im_G);<span class="comment">%求一阶图像</span></span><br><span class="line">im_edge = <span class="built_in">abs</span>(im_edge).^p;<span class="comment">%闵可夫斯基p范式</span></span><br><span class="line"></span><br><span class="line">r = im_edge(:,:,<span class="number">1</span>);</span><br><span class="line">g = im_edge(:,:,<span class="number">2</span>);</span><br><span class="line">b = im_edge(:,:,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">Avg = <span class="built_in">mean</span>(im_edge(:)).^(<span class="number">1</span>/p);<span class="comment">%计算出来的光照颜色</span></span><br><span class="line"></span><br><span class="line">R_avg = mean2(r).^(<span class="number">1</span>/p);<span class="comment">%各通道</span></span><br><span class="line">G_avg = mean2(g).^(<span class="number">1</span>/p);</span><br><span class="line">B_avg = mean2(b).^(<span class="number">1</span>/p);</span><br><span class="line"></span><br><span class="line">k = [R_avg G_avg B_avg]./Avg;<span class="comment">%增益k</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">3</span></span><br><span class="line">out(:,:,<span class="built_in">i</span>) = im(:,:,<span class="built_in">i</span>)/k(<span class="built_in">i</span>);</span><br><span class="line">out(:,:,<span class="built_in">i</span>) = <span class="built_in">min</span>(out(:,:,<span class="built_in">i</span>),<span class="number">1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="结果比较"><a href="#结果比较" class="headerlink" title="结果比较"></a>结果比较</h4><p><img src="https://img-blog.csdnimg.cn/20190930213825195.jpg#pic_center" alt=""></p><h4 id="一般化计算框架"><a href="#一般化计算框架" class="headerlink" title="一般化计算框架"></a>一般化计算框架</h4><p>&emsp;&emsp;针对以上几种方法，在<a href="http://colorconstancy.com/" target="_blank" rel="noopener">Color Constancy</a>网站上给出了一般化框架代码，写的稍微有点复杂，就也没看很仔细，点击<a href="https://sl-m-ssl.xunlei.com/h5/page/download-share/index.html?entry=link&amp;appType=PC&amp;videobtindex=-1&amp;storid=50597fa04d" target="_blank" rel="noopener">源代码下载</a>。</p><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p>[1]王金华, 李兵, 须德. 图像理解:颜色认知计算[M]. 清华大学出版社, 2013.</p><p>[2] <a href="http://colorconstancy.com/" target="_blank" rel="noopener">Color Constancy</a></p>]]></content>
      
      
      <categories>
          
          <category> 白平衡 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白平衡算法之Gray World、White Patch、SoG</title>
      <link href="/posts/26a617fd.html"/>
      <url>/posts/26a617fd.html</url>
      
        <content type="html"><![CDATA[<p>开坑记录一下白平衡相关算法，从几个最基本的白平衡算法开始。<a href="https://blog.csdn.net/weixin_43194305/article/details/101758864" target="_blank" rel="noopener">CSDN原始发布</a>。</p><a id="more"></a><h2 id="白平衡算法之Gray-World和White-Patch"><a href="#白平衡算法之Gray-World和White-Patch" class="headerlink" title="白平衡算法之Gray World和White Patch"></a>白平衡算法之Gray World和White Patch</h2><p>&emsp;&emsp;颜色作为物体最基本的属性，在大部分场合对人类视觉而言是一个能够轻而易举捕获的信息。但在数字成像过程中，颜色是一种及其不稳定的图像特征。数字成像时获取的颜色主要依赖三个因素：物体表面光谱反射率、场景中的光照条件和成像器件的对光的灵敏度曲线。人类视觉系统存在一种颜色恒常性功能，能够在不同的光照下自动消除光照的影响，从而获得较稳定和准确的物体颜色。但是成像设备不具备这种特质，因此白平衡算法对成像颜色好坏起到了至关重要的作用。</p><p>&emsp;&emsp;白平衡算法的核心内容就是估计光照。但是从一幅已有图像去估计光照本身就是一个病态问题，因此，现有的白平衡算法基本都是基于一定的假设和先验条件。其中，Gray World和White Patch是两个基于各自假设，简单、实用的白平衡算法。</p><hr><h3 id="White-Patch（MaxRGB）"><a href="#White-Patch（MaxRGB）" class="headerlink" title="White Patch（MaxRGB）"></a>White Patch（MaxRGB）</h3><p>&emsp;&emsp;White Patch假设：图像中，RGB颜色通道的最大响应是由场景中的白色表面引起的。理论上白色表面可以反射场景光照的颜色，因此，RGB通道中最大的值将被作为图像的光照颜色。所以，该算法又被称为MaxRGB算法。其数学形式为：</p><script type="math/tex; mode=display">\max\limits_{x}f(X)=ke\tag1</script><p>&emsp;&emsp;其中$X$表示像素点坐标，$k$为用于校准光照的常量，$e$为光照。该最大值是各通道分别计算，而不是必须是一个像素点的三通道最大。这就导致了该算法在很多场合中其实并不适用，因为该算法假设需要在场景中存在一个白色（标准光源下）像素点或者三通道反射率相同的点（灰点）。当场景中没有这样的点的时候，该算法的表现就会比较糟糕。而以后的许多算法也是在找白点上做了各种改进，该算法最大的优点就是简单高效。</p><p>&emsp;&emsp;具体的实现也比较简单，实际用的时候，白点的定义也可以有很多。归一化后可以以1作为最大点，也可以用绿色通道最大值作为最亮点求各个通道的增益系数。</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ul><li>计算三通道各自最大值$R_{max}$，$G_{max}$，$B_{max}$。</li><li>计算增益，$k=[R_{max},G_{max},B_{max}]./Max$，也可以直接以1或者$G_{max}$作为光照。</li><li>对原图三个通道乘上增益，$R/k,G/k,B/k$。</li></ul><h4 id="Matlab代码"><a href="#Matlab代码" class="headerlink" title="Matlab代码"></a>Matlab代码</h4><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">out</span>=<span class="title">MaxRGB</span><span class="params">(im)</span></span></span><br><span class="line"><span class="comment">% Image should be normalized to 0-1 归一化</span></span><br><span class="line"></span><br><span class="line">R_max = <span class="built_in">max</span>(<span class="built_in">max</span>(im(:,:,<span class="number">1</span>)));</span><br><span class="line">G_max = <span class="built_in">max</span>(<span class="built_in">max</span>(im(:,:,<span class="number">2</span>)));</span><br><span class="line">B_max = <span class="built_in">max</span>(<span class="built_in">max</span>(im(:,:,<span class="number">3</span>)));</span><br><span class="line">Max = <span class="built_in">max</span>(im(:));</span><br><span class="line"></span><br><span class="line">k = [R_max G_max B_max]./Max;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    out(:,:,<span class="built_in">i</span>) = im(:,:,<span class="built_in">i</span>)/k(<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p><img src="https://img-blog.csdnimg.cn/20190930135051762.png#pic_center" alt=""></p><hr><h3 id="Gray-World"><a href="#Gray-World" class="headerlink" title="Gray World"></a>Gray World</h3><p>&emsp;&emsp;Gray World假设：场景中所有物理表面的平均反射是无色差的（灰色的）。这也是灰色世界名字的由来，简而言之，Gray World就是将整幅图像的平均颜色作为图像的光照颜色。Gray World的假设条件相比MaxRGB相对宽松，对一般图像适应能力强，同时也很简单，因此得到广泛使用。其缺点是，当图像中颜色比较单一的时候，该法就会失效。对其的改进也是主要基于如何适用于颜色较单一场景的情况，例如对图像分块处理。</p><p>&emsp;&emsp;数学形式如下：</p><script type="math/tex; mode=display">\frac{\int f(X)dX}{\int dX}=ke\tag2</script><h4 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h4><ul><li>计算三通道各自平均值$R_{avg}$，$G_{avg}$，$B_{avg}$，整个图像的平均值$ALL_{avg}$作为光源$e$。</li><li>计算增益，$k=[R_{avg},G_{avg},B_{avg}]/ALL_{avg}$，同样的，光源$e$也可以按需求取0.5，$G_{avg}$等。</li><li>对原图三个通道除以增益，$R/k,G/k,B/k$。</li></ul><h4 id="Matlab代码-1"><a href="#Matlab代码-1" class="headerlink" title="Matlab代码"></a>Matlab代码</h4><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">out</span>=<span class="title">GrayWorld</span><span class="params">(im, flag)</span></span></span><br><span class="line"><span class="comment">% Image should be normalized to 0-1 归一化</span></span><br><span class="line"></span><br><span class="line">R_avg = mean2(im(:,:,<span class="number">1</span>));</span><br><span class="line">G_avg = mean2(im(:,:,<span class="number">2</span>));</span><br><span class="line">B_avg = mean2(im(:,:,<span class="number">3</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ~exist(<span class="string">'flag'</span>,<span class="string">'var'</span>)||flag==<span class="number">0</span></span><br><span class="line">    Avg = <span class="number">0.5</span>;</span><br><span class="line"><span class="keyword">elseif</span> flag==<span class="number">1</span></span><br><span class="line">    Avg = mean2(im);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    Avg = G_avg;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">k = [R_avg G_avg B_avg]./Avg;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">3</span></span><br><span class="line">out(:,:,<span class="built_in">i</span>) = im(:,:,<span class="built_in">i</span>)/k(<span class="built_in">i</span>);</span><br><span class="line">out(:,:,<span class="built_in">i</span>) = <span class="built_in">min</span>(out(:,:,<span class="built_in">i</span>),<span class="number">1</span>);<span class="comment">%处理一下有可能超出1的值，直接设1</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h4><p><img src="https://img-blog.csdnimg.cn/20190930135019800.png#pic_center" alt=""></p><hr><h3 id="SoG-Shades-of-Gray"><a href="#SoG-Shades-of-Gray" class="headerlink" title="SoG(Shades of Gray)"></a>SoG(Shades of Gray)</h3><p>&emsp;&emsp;为了将Gray World更加一般化，Finalayson等人在上式中引入了闵可夫斯基范式(Minkowskinorm)，提出了一种SoG算法。SoG算法利用闵式距离代替简单求平均的方法，其数学形式如下：</p><script type="math/tex; mode=display">(\frac{\int (f(X))^pdX}{\int dX})^{1/p}=ke\tag3</script><p>该算法将MaxRGB和GrayWorld算法纳入了同一个计算框架下：</p><ul><li>当$p=1$时，该式就退化为GrayWorld算法，直接求图像平均。</li><li>当$p=\infty$时，该式等价于求$f(X)$最大值，等同于MaxRGB法。</li><li>当$1&lt;p&lt;\infty$时，就是普通的SoG算法，Finalayson等指出，在$p=6$时，算法取得较好的适用性和效果。</li></ul><h4 id="Matlab代码-2"><a href="#Matlab代码-2" class="headerlink" title="Matlab代码"></a>Matlab代码</h4><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">out</span>=<span class="title">SoG</span><span class="params">(im, p)</span></span></span><br><span class="line"><span class="comment">% Image should be normalized to 0-1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ~exist(<span class="string">'p'</span>,<span class="string">'var'</span>)</span><br><span class="line">    p=<span class="number">6</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">imP = im.^p;</span><br><span class="line"></span><br><span class="line">R_avg = mean2(imP(:,:,<span class="number">1</span>)).^(<span class="number">1</span>/p);</span><br><span class="line">G_avg = mean2(imP(:,:,<span class="number">2</span>)).^(<span class="number">1</span>/p);</span><br><span class="line">B_avg = mean2(imP(:,:,<span class="number">3</span>)).^(<span class="number">1</span>/p);</span><br><span class="line"></span><br><span class="line">Avg = mean2(imP).^(<span class="number">1</span>/p);</span><br><span class="line">    </span><br><span class="line">k = [R_avg G_avg B_avg]./Avg;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">3</span></span><br><span class="line">out(:,:,<span class="built_in">i</span>) = im(:,:,<span class="built_in">i</span>)/k(<span class="built_in">i</span>);</span><br><span class="line">out(:,:,<span class="built_in">i</span>) = <span class="built_in">min</span>(out(:,:,<span class="built_in">i</span>),<span class="number">1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h4><p><img src="https://img-blog.csdnimg.cn/20190930134926282.png#pic_center" alt=""></p><h3 id="更加一般化的框架："><a href="#更加一般化的框架：" class="headerlink" title="更加一般化的框架："></a>更加一般化的框架：</h3><a href="/posts/df87526a.html" title="GrayEdge">GrayEdge</a><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1]王金华, 李兵, 须德. 图像理解:颜色认知计算[M]. 清华大学出版社, 2013.</p>]]></content>
      
      
      <categories>
          
          <category> 白平衡 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>暗通道先验去雾</title>
      <link href="/posts/2a35425d.html"/>
      <url>/posts/2a35425d.html</url>
      
        <content type="html"><![CDATA[<p>本文首发于<a href="https://blog.csdn.net/weixin_43194305/article/details/89206379" target="_blank" rel="noopener">CSDN</a></p><h2 id="暗通道先验"><a href="#暗通道先验" class="headerlink" title="暗通道先验"></a>暗通道先验</h2><p>暗通道先验是基于如下观察，在户外的无雾图像中，在大部分非天空区域，至少有一个通道值是很小一个数或趋近于零。因此，对任意一幅图$J$，给出暗通道$J^{dark}$的表示：</p><script type="math/tex; mode=display">J^{dark}(X)=\min _{y\in \Omega(x)}(\min _{c\in \{r, g, b\}}J^c(Y)),\tag 1</script><p>其中两个最小是各通道最小，局部窗口最小。即首先对图像每个像素取三通道中最小值，得到一个单通道图，然后对这个单通道图作最小值滤波就可以得到暗通道图$J^{dark}$。<br>作者将造成这个现象的原因归结为以下三点：</p><ul><li>各类物体的阴影，玻璃</li><li>彩色物体表面，如花草树木，蓝色的水面</li><li>黑色物体表面，如树干，石头等</li></ul><p>正因为自然界总是充满了彩色和阴影，就导致了图像暗通道总是很暗。为了验证这个先验知识，作者统计了大量图片，发现基本都符合这个先验。以下是几幅680*1024的风景图在不同大小滤波窗口下的暗通道图：<img src="https://img-blog.csdnimg.cn/20190411124336347.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_123456,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190411124400882.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_123456,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190411124429584.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_123456,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190411124448455.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_123456,t_70" alt="在这里插入图片描述"><br>以上图像基本都符合暗通道先验，由此可见暗通道的普遍性。在暗通道先验的基础上，就可以进行去雾算法的推导。</p><hr><h2 id="雾模型分析"><a href="#雾模型分析" class="headerlink" title="雾模型分析"></a>雾模型分析</h2><p>在计算机视觉和计算机图形领域，一个常用来描述有雾图像的公式表达为：</p><script type="math/tex; mode=display">I(x)=J(x)t(x)+A(1-t(x))\tag2</script><p>其中，$I$表示有雾图像，$J$是要恢复的无雾的图像，$A$是全球大气光成分， $t(x)$为透射率。<br>&#8195;&#8195;在这里主要剖析一下这个式子。对这个式子，本身的理解，大气光成分和图像中的景物本身就是真实存在的。晴天和雾天的区别只是大气光成分的多少，表达在这个式子里就是透射率。晴天的时候大气光成分少，物体反射光的透射率很高，几乎让人感受不到大气光成分的存在。雾天则相反。<br>&#8195;&#8195;放在PS中理解，该模型几乎就是两个图层在不同透明度下的叠加。可以设透明度为$\alpha$。叠加后的图像为<script type="math/tex">I=\alpha I_1+(1-\alpha)I_2</script>因此，我们假设最大大气光成分为255，可以通过设置不同的透射率来产生不同的有雾图像，同样用上述的图片作实验。<br><img src="https://img-blog.csdnimg.cn/20190411161818601.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_123456,t_70" alt="在这里插入图片描述"><br>不过，由于人为选定的$t(x)$在每个像素上都是一致的，所以会丢失景深的感觉。但是已经足以说明雾的生成，和去雾的思路。我们已知雾图，由式（2）去求解无雾的图像。同时，我们用无雾图的暗通道和人为生成$t(x)=0.5$的雾图暗通道进行对比。<br><img src="https://img-blog.csdnimg.cn/20190411163043821.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_123456,t_70" alt="在这里插入图片描述"></p><hr><h2 id="基于暗通道的去雾算法"><a href="#基于暗通道的去雾算法" class="headerlink" title="基于暗通道的去雾算法"></a>基于暗通道的去雾算法</h2><p><font face="微软雅黑">首先假设大气光成分A已知。去雾模型（2）可以化为以下方程：<script type="math/tex">\frac{I^c(x)}{A^c}=t(x)\frac{J^c(x)}{A^c}+1-t(x).\tag3</script></p><p>上标$c$即表示r,g,b三通道。进一步假设每个滤波窗口内的透射率$t(x)$是常数，记为$\bar t(x)$。然后对方程两边同时计算暗通道，即作两次最小值运算，可得下式：</p><script type="math/tex; mode=display">\min_{y\in \Omega(x)}(\min_c\frac{I^c(y)}{A^c})=\bar t(x)\min_{y\in \Omega (x)}(\min_c\frac{J^c(y)}{A^c})+1-\bar t(x).\tag4</script><p>因为$\bar t(x)$是常量，因此放在最小运算外面。<br>根据暗通道先验，$J$趋近于零：</p><script type="math/tex; mode=display">J^{dark}(x)=\min_{y\in\Omega (x)}(\min_cJ^c(y))=0\tag5</script><p>因为$A^c$总是正值，可得：</p><script type="math/tex; mode=display">\min_{y\in \Omega (x)}(\min_c\frac{J^c(y)}{A^c})=0.\tag6</script><p>将式(6)代回式(4)，即可简单地得到透射率估计值：</p><script type="math/tex; mode=display">\bar t(x)=1-\min_{y\in \Omega(x)}(\min_c\frac{I^c(y)}{A^c}).\tag7</script><p>同时，即使是晴天，大气光成分还是存在的，尤其是在看远处的物体时给人的感觉更强。这种大气光成分会给人一种景深的层次感，去雾要有所保留。因此，引入一个常量参数$\omega (0&lt;\omega&lt;1)$用来控制去雾的程度：</p><script type="math/tex; mode=display">\bar t(x)=1-\omega\min_{y\in \Omega(x)}(\min_c\frac{I^c(y)}{A^c}).\tag8</script><p>作者在文中建议的$\omega$为0.95。<br>&#8195;&#8195;在算法开始的地方就假设$A$是已知，那么具体如何得到A的值。作者在文中给出的方法是，在暗通道中找出前0.1%最亮的点，即透射率最小的点。对于这些点，去雾图中找到对于的点，并取它们中的所有通道最大的值作为$A$的近似。至此，透射率$t(x)$，大气光成分$A$，雾图$I$，都是已知了，就可以求解无雾图：</p><script type="math/tex; mode=display">J(x)=\frac{I(x)-A}{\max(t(x),t_0)}+A\tag9</script><p>其中，$t_0$为一个透射率下界。由于直接恢复时，当透射率$t(x)$接近零的时候，由式(2)可知，$J(x)t(x)$也为零，这就会失去原图信息，容易引入噪声，因此设置一个下界，在雾密度很大的地方，保留一定数量的雾。$t_0$的值一般取0.1。作者还提到，去雾后的图像一般会显得比较暗淡，可以适当增加曝光以得到更好的效果。</p><hr><h2 id="导向滤波"><a href="#导向滤波" class="headerlink" title="导向滤波"></a>导向滤波</h2><p><font face="微软雅黑">&#8195;&#8195;以原图灰度图作为导向图，对透射率图进行导向滤波，可以得到非常精细的透射率图，从而得到高质量的去雾图。这里再以之前人为生成的“雾图”说明导向滤波的效果。<br><img src="https://img-blog.csdnimg.cn/2019041121200278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_123456,t_70" alt="在这里插入图片描述"></p>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
          <category> 图像处理 </category>
          
          <category> 去雾 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 去雾 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Guided Filter(引导滤波)</title>
      <link href="/posts/e5423594.html"/>
      <url>/posts/e5423594.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/weixin_43194305/article/details/88959183" target="_blank" rel="noopener">原文首发于CSDN</a></p><h2 id="导向滤波"><a href="#导向滤波" class="headerlink" title="导向滤波"></a>导向滤波</h2><blockquote><p>Guided Image Filtering - <a href="http://kaiminghe.com/" target="_blank" rel="noopener">He Kaiming</a> 2009</p></blockquote><p><font face="微软雅黑">&#8195;&#8195;导向滤波（Guided Filtering）和双边滤波（BF）、最小二乘滤波（WLS）是三大边缘保持（Edge-perserving）滤波器。当然，引导滤波的功能不仅仅是边缘保持，只有当引导图是原图的时候，它就成了一个边缘保持滤波器。<br>&#8195;&#8195;它在图像去雾，图像抠图上均有相应的应用。</p><hr><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><div align=center><img  src="https://img-blog.csdnimg.cn/20190430123252187.jpg"/></div>&#8195;&#8195;对于一个输入的图像$p$，通过引导图像$I$，经过滤波后得到输出图像$q$，其中$p$和$I$都是算法的输入。引导滤波定义了如下所示的一个线性滤波过程，对于$i$位置的像素点，得到的滤波输出是一个加权平均值：$$q_i=\sum_j W_{ij}(I)pj, \tag1$$其中，$i$和$j$分别表示像素下标。$W_{ij}$是只和引导图像$I$相关的滤波核。该滤波器相对于$p$是线性的。&#8195;&#8195;导向滤波的一个重要假设是输出图像$q$和引导图像$I$在滤波窗口$w_k$上存在局部线性关系：$$q_i=a_kI_i+b_k,\forall i\in w_k,\tag2$$对于一个以$r$为半径的确定的窗口$w_k$，（$a_k$，$b_k$）也将是唯一确定的常量系数。这就保证了在一个局部区域里，如果引导图像$I$有一个边缘的时候，输出图像$q$也保持边缘不变，因为对于相邻的像素点而言，存在$\nabla q=a\nabla I$。因此只要求解得到了系数$a$，$b$也就得到了输出$q$。同时认为输入图像中非边缘区域又不平滑的地方视为噪声$n$，就有$q_i=p_i-n_i$。最终的目标就是最小化这个噪声。对于每一个滤波窗口，该算法在最小二乘意义上的最优化可表示为$$argmin \sum_{i\in w_k}(q_i-p_i)^2 \\\\ argmin \sum_{i\in w_k}(a_kI_i+b_k-p_i)^2 \tag3$$最后，引入一个正则化参数$\epsilon$避免$a_k$过大，得到滤波窗口内的损失函数：$$E(a_k,b_k)=\sum_{i\in w_k}((a_kI_i+b_k-p_i)^2+\epsilon a_k^2).\tag4$$求解最优化过程（对参数求偏导）：$$\frac {\delta E}{a_k}=\sum_{i\in w_k}(2(a_kI_i+b_k-p_i)(I_i)+2\epsilon a_k)=0 $$$$ \frac {\delta E}{b_k}=\sum_{i\in w_k}(2(a_kI_i+b_k-p_i))=0$$$$a_k =\frac{\sum_{i\in w_k}p_iI_i-b_k\sum_{i\in w_k}I_i}{\sum_{i\in w_k}(I_i+\epsilon)} $$$$ b_k=\sum_{i\in w_k}p_i-a_k\sum_{i\in w_k}I_i$$将$b_k$代入$a_k$，整理可得：$$a_k =\cfrac{\cfrac{1}{\left| w\right|}\sum_{i\in w_k}I_ip_i-\mu _k\bar p_k}{\sigma _k^2+\epsilon}\tag5$$$$b_k = \bar p_k-a_k\mu_k.\tag6$$在这里，$\mu_k$和$\sigma_k^2$分别表示引导图像$I$在窗口$w_k$中的平均值和方差，$|w|$是窗口$w_k$中像素点的个数，$\bar p_k=\frac{1}{|w|}\sum_{i\in w_k}p_i$是输入图像在窗口$w_k$中的平均值。<font face="微软雅黑">&#8195;&#8195;接下来，只要把上述线性模型应用到整个图像的滤波窗口。但是可以看到，每一个像素点会被包含在多个窗口里。比如，如果用3*3的窗口滤波，那么除了边缘区域的每个点都会被包含在9个窗口里。因此，对于不同的窗口，我们将会得到$|w|$个$q_i$值，就对所有的$q_i$值取平均，得到最终结果：$$q_i=\frac{1}{|w|}\sum_{k:i\in w_k}(a_kI_i+b_k)\tag7$$$$\ \ =\bar a_iI_i+\bar b_i\tag8$$其中$\bar a_i=\frac{1}{|w|}\sum_{k:i\in w_k}a_k$，$\bar b_i=\frac{1}{|w|}\sum_{k:i\in w_k}b_k$。由此建立了每个像素点从$I$到$q$的映射。***## 边缘保持<font face="微软雅黑">&#8195;&#8195;对于该算法，当$I=p$时，即输入图像和引导图像是同一副图像时，该算法即成为一个边缘保持滤波器。同时，方程的解也可作如下表示：$$a_k =\cfrac{\sigma _k^2}{\sigma _k^2+\epsilon}$$$$b_k = (1-a_k)\bar p_k$$从中可以看出，$\epsilon$在这里相当于界定平滑区域和边缘区域的阈值。考虑以下两种情况：- Case 1：平坦区域。如果在某个滤波窗口内，该区域是相对平滑的，方差$\sigma _k^2$将远远小于$\epsilon$。从而$a_k\approx0,b_k\approx\bar p_k$。相当于对该区域作均值滤波。- Case 2：高方差区域。相反，如果该区域是边缘区域，方差很大，$\sigma _k^2$将远远大于$\epsilon$。从而$a_k\approx1,b_k\approx0$。相当于在区域保持原有梯度。&#8195;&#8195;***## 应用### 1、以自身作为引导图的保边平滑滤波：<div align=center><img  src="https://img-blog.csdnimg.cn/20190430132407202.jpg"/></div><h3 id="2、以原图引导的对透射率滤波的暗通道去雾"><a href="#2、以原图引导的对透射率滤波的暗通道去雾" class="headerlink" title="2、以原图引导的对透射率滤波的暗通道去雾"></a>2、<a href="https://blog.csdn.net/weixin_43194305/article/details/89206379" target="_blank" rel="noopener">以原图引导的对透射率滤波的暗通道去雾</a></h3><h3 id="3、以原图引导的对权重图滤波的引导图像融合"><a href="#3、以原图引导的对权重图滤波的引导图像融合" class="headerlink" title="3、以原图引导的对权重图滤波的引导图像融合"></a>3、<a href="https://blog.csdn.net/weixin_43194305/article/details/90678312" target="_blank" rel="noopener">以原图引导的对权重图滤波的引导图像融合</a></h3>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于WLS和BF的RGB-NIR图像融合</title>
      <link href="/posts/37d5cb50.html"/>
      <url>/posts/37d5cb50.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文：<a href="https://www.researchgate.net/publication/318676335_RGB-NIR_Image_Enhancement_by_Fusing_Bilateral_and_Weighted_Least_Squares_Filters?ev=auth_pub" target="_blank" rel="noopener">RGB-NIR Image Enhancement by Fusing Bilateral and Weighted Least Squares Filters</a> - 2017</p><p>BF：<a href="https://users.soe.ucsc.edu/~manduchi/Papers/ICCV98.pdf" target="_blank" rel="noopener">Bilateral Filtering for Gray and Color Images</a> - 1998</p><p>WLS：<a href="http://www.cs.huji.ac.il/~danix/epd/" target="_blank" rel="noopener">Edge-Preserving Decompositions for Multi-Scale Tone and Detail Manipulation</a> - 2008</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>&#160; &#160; &#160; &#160;本文利用双边滤波器和加权最小二乘滤波器对RGB图像和NIR图像进行融合，核心思想即提取NIR图像中的细节层，与之将RGB的基础层，即相对平滑的部分相加，得到融合后的图片。</p><p>&#160; &#160; &#160; &#160;对于BF和WLS不甚了解的，可以自行查找相关资料，加权最小二乘滤波WLS（weighted least squares）加上双边滤波，引导滤波是三种较为经典的边缘保持性滤波算法，其改进方法和相关资料也有很多。本文仅使用BF和WLS。WLS中的数学描述也值得品味，在此贴出几篇关于WLS的参考博文。</p><p><r></r><br>&#160; &#160; &#160; &#160;[1:<a href="https://blog.csdn.net/piaoxuezhong/article/details/78396498" target="_blank" rel="noopener">https://blog.csdn.net/piaoxuezhong/article/details/78396498</a>]<br>&#160; &#160; &#160; &#160;[2:<a href="https://blog.csdn.net/victoriaw/article/details/71171813" target="_blank" rel="noopener">https://blog.csdn.net/victoriaw/article/details/71171813</a>]</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>&#160; &#160; &#160; &#160;在这两个滤波器的支持下，本文的实现就较为简单了。作者将本文的方法称为<strong>BFWLS</strong>，本质上只是两种方法的结合。作者提出BF在某一尺度上具有很好的提取能力，而WLS在多尺度细节上的提取能力更强，综合两者的优点，可以提取更多潜在的细节。如下是作者在论文中贴出的操作流程。<br><img src="https://img-blog.csdnimg.cn/20190320111648231.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>&#160; &#160; &#160; &#160;此处，由于RGB图像是在YCbCr颜色空间上操作了，这里的Y代表了图像的亮度信息。因此，我们可以将NIR图像也视为强度信息或者亮度信息。后面用到的$d$指图像细节层（detail），$b$指基础层（base）。<br>&#160; &#160; &#160; &#160;<strong>Step1</strong>：此处将NIR图像标记为<kbd>$Y_{nir}$</kbd>，分别用两个滤波器对NIR图像滤波，将会得到两张滤波后的平滑图像<kbd>$Y_{WLS}^b$</kbd>和<kbd>$Y_{BF}^b$</kbd>。而细节层的图像可以分别简单得通过原图减去基础层的图像得到，即<kbd>$I_{nir}-Y_?^b$</kbd>得到<kbd>$Y_{WLS}^d$</kbd>和<kbd>$Y_{BF}^d$</kbd>。对两个细节层做一次平均得到<kbd>$Y^d$</kbd>，这就是我们从NIR图像中提取出来的细节信息。<br>&#160; &#160; &#160; &#160;<strong>Step2</strong>：将RGB图像转到YCbCr空间（<a href="https://baike.baidu.com/item/YCbCr/10012133?fr=aladdin" target="_blank" rel="noopener">YCbCr</a>），单独拿出Y层，对其做一次WLS滤波，滤波结果为<kbd>$Y_{WLS}^b$</kbd>，与第一步得到的<kbd>$Y^d$</kbd>相加就是我们融合后新图像在YCbCr空间下的Y层。<br>&#160; &#160; &#160; &#160;<strong>Step3</strong>：将新的Y层和原先的CbCr层重新组合并转化回RGB空间。就是本算法的结果，下面看一下融合效果。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>RGB：<br><img src="https://img-blog.csdnimg.cn/20190320184607907.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_FFFFFF,t_70" alt="RGB图像"><br>NIR：<br><img src="https://img-blog.csdnimg.cn/2019032018462331.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE5NDMwNQ==,size_16,color_FFFFFF,t_70" alt="NIR图像"><br>融合后：<br><img src="https://img-blog.csdnimg.cn/20190430152516484.png" alt="在这里插入图片描述"><br>对比：<br><img src="https://img-blog.csdnimg.cn/20190430152610947.png" alt="在这里插入图片描述"><br>&#160;&#160;&#160;&#160;==<strong>可以明显得看到，远山和天空云朵的细节被增强了。</strong>==</p><p><r></r></p><p><r></r></p><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p><a href="https://blog.csdn.net/weixin_43194305/article/details/93035340" target="_blank" rel="noopener">图像融合专题</a><br> <a href="http://matthewalunbrown.com/nirscene/nirscene.html" target="_blank" rel="noopener">RGB+NIR数据集下载</a><br> <a href="https://github.com/entropyzeroo/ImageFusion" target="_blank" rel="noopener">代码</a></p>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
          <category> 图像融合 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NIR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/4a17b156.html"/>
      <url>/posts/4a17b156.html</url>
      
        <content type="html"><![CDATA[<h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h3><p>Hello！你好！こんにちは！안녕하세요！</p><a id="more"></a>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 开天辟地 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
